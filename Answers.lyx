#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title

\bar under
APML Project - Snake
\end_layout

\begin_layout Author

\bar under
Vitaly Rajev
\bar default
 -
\end_layout

\begin_layout Author

\bar under
Ziv Mahluf
\bar default
 -
\end_layout

\begin_layout Section*
Part 1 - Linear Agent
\end_layout

\begin_layout Subsection*
1.
 Board Representation:
\end_layout

\begin_layout Standard
We chose to represent the board by using a vector of indicators on a window
 of a given radius around the position of the snake's head on the board,
 meaning we took a window around the snake's head, for each pixel in the
 window we had 11 (as the number of possible values) elements in the representat
ion vector, and we set 1 in the position corresponding to the value on the
 board and left the rest as 0.
\end_layout

\begin_layout Standard
We chose this form of representation since it allowed the agent, which calculate
s the q-value of a state using a dot product of a weights vector and the
 representation of the state, to adjust weights to specific values in specific
 positions, allowing it to approximate the q-function more precisely.
\end_layout

\begin_layout Subsection*
2.
 Learning Algorithm:
\end_layout

\begin_layout Standard
The learning algorithm used for the linear agent is similar to the standard
 q-learning algorithm with a modification on the update step, since the
 agent doesn't manage a q-table and instead approximates the q-values using
 a linear feature function, the weights vector is updated as follows: 
\begin_inset Formula $weights=(1-\alpha)\cdot weights+\alpha\cdot(reward+\gamma\cdot max_{a\in Actions}\{Q(prev\_state,a)\})\cdot_{element-wise}\phi(prev\_state)$
\end_inset

 (Where 
\begin_inset Formula $\alpha$
\end_inset

 is the learning rate, and 
\begin_inset Formula $\gamma$
\end_inset

 is the discount factor).
\end_layout

\begin_layout Standard
Meaning, we update the weights vector such that the weights corresponding
 to the previous state are updated according to 
\begin_inset Formula $\alpha$
\end_inset

, 
\begin_inset Formula $\gamma$
\end_inset

, and the maximal q value achievable from the previous state, and the rest
 of the vectors are multiplied by 
\begin_inset Formula $(1-\alpha)$
\end_inset

.
\end_layout

\begin_layout Section*
Part 2 - Custom Agent
\end_layout

\begin_layout Subsection*
1.
 Board Representation:
\end_layout

\begin_layout Subsection*
2.
 Custom Model Details:
\end_layout

\begin_layout Subsection*
3.
 Learning Algorithm:
\end_layout

\begin_layout Standard
The learning algorithm used for the custom agent consists of two neural
 networks with an identical architecture, a target network which is seen
 as an expert, and a training network, which constantly trains to mimic
 the expert.
\end_layout

\begin_layout Standard
Both networks are initialized randomly, and in each learning iteration,
 the target network predicts the q values of the resulting states possible
 from the current state (for each action).
 To this prediction, in the index of the action taken in the previous state,
 we add 
\begin_inset Formula $(reward+\gamma\cdot max(training\_network(prev\_state)))$
\end_inset

, to mimic the q-learning update step for a state and an action, and then
 we fit the training network with the previous state as an input, and the
 predictions resulting from the target network's prediction, with the addition
 to the corresponding action, as the label.
\end_layout

\begin_layout Standard
In addition, every few learning rounds, the target network's weights were
 updated to a weighted average of its current weights, and the learning
 network's current weights.
 This is done in order for the target network to improve (since the learning
 network is trained to predict according to the target network, and the
 update step of the q-learning algorithm) from what the training network
 learns, and gradually decrease the weight of the randomness in the initializati
on.
\end_layout

\begin_layout Subsection*
4.
 Testing the Custom Agent:
\end_layout

\begin_layout Subsection*
5.
 Additional Considered Solutions:
\end_layout

\begin_layout Subsection*
6.
 Exploration-Exploitation Tradeoff:
\end_layout

\end_body
\end_document
