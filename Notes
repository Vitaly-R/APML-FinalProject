Current Tasks:
* current custom is learning very slowly - why? how does each hyper parameter effect the result? can we limit the hyper
    parameters better? can we use more techniques?
* find the optimal hyperparameters



TODOs:  (after current)

* check values of q function -- does it reach nan values because it gets too big?

* find a way to use decay on learning rate while having constant MIN learning rate

* if we're score_scope away from the end - stop exploration to maximize results



USEFUL SOURCES:
https://github.com/keon/deep-q-learning/blob/master/dqn.py

https://github.com/keon/deep-q-learning





Tuesday morning:

* make epsilon min higher -- will let you use random acts at later rounds  +++++++++++

* update target only once every couple of learns  +++++++++++++++++

* start decay only after a while of random acts - not from first learn +++++++++

